AKS - BASIC TO ADVANCED
================================================================================================================================================================
#Local Variables

baseFolderPath="/Users/monojitdattams/Development/Projects/Workshops/AKSWorkshop/AKSTraining-Basic-to-Pro/Deployments"
setupFolderPath="$baseFolderPath/Setup"
microservicesFolderPath="$baseFolderPath/Microservices"
tenantId="72f988bf-86f1-41af-91ab-2d7cd011db47"
subscriptionId="6bdcc705-8db6-4029-953a-e749070e6db6"
aksResourceGroup="aks-train-rg"
masterResourceGroup="master-workshop-rg"
location="eastus"
clusterName="aks-train-cluster"
version="1.20.7"
acrName="akstrnacr"
acrId=
keyVaultName="aks-train-kv"
keyvaultId=
masterVnetName="master-workshop-vnet"
masterVnetPrefix="11.0.0.0/16"
masterVnetId=
masterSubnetName="master-js-ubuntuvm-subnet"
masterSubnetPrefix="11.0.1.32/27"
masterSubnetId=
aksVnetName="aks-train-vnet"
aksVnetPrefix="18.0.0.0/21"
aksVnetId=
aksSubnetName="aks-train-subnet"
aksSubnetPrefix="18.0.0.0/24"
aksSubnetId=
aksIngressSubnetName="aks-train-ingress-subnet"
aksIngressSubnetPrefix="18.0.1.0/24"
aksIngressSubnetId=
aksServicePrefix="18.0.2.0/24"
dnsServiceIP="18.0.2.10"
appgwName="aks-train-appgw"
appgwSubnetName="aks-train-appgw-subnet"
appgwSubnetPrefix="18.0.3.0/27"
appgwSubnetId=
apimName=""
apimSubnetName=""
apimSubnetPrefix=""
apimSubnetId=
sysPoolName=akssyspool
sysPoolNodeSize="Standard_DS2_v2"
sysPoolNodeCount=3
sysPoolMaxPods=30
sysPoolMaxNodeCount=5
apiPoolName=aksapipool
apiPoolNodeSize="Standard_DS2_v2"
apiPoolNodeCount=3
apiPoolMaxPods=30
apiPoolMaxNodeCount=5
networkPlugin=azure
networkPolicy=azure
vmSetType=VirtualMachineScaleSets
osType=Linux
addons=monitoring
masterAKSPeering="$masterVnetName-$aksVnetName-peering"
aksMasterPeering="$aksVnetName-$masterVnetName-peering"
masterPrivateDNSLink="$masterVnetName-dns-plink"
aksPrivateDNSLink="$aksVnetName-dns-plink"
aksIngControllerName="aks-train-ing"
aksIngControllerNSName="$aksIngControllerName-ns"
aksIngControllerFileName="internal-ingress"
aksIngControllerFilePath="$baseFolderPath/Setup/Common/internal-ingress.yaml"
privateDNSZoneName="internal.wkshpdev.com"
privateDNSZoneId=
backendIpAddress=
aadAdminGroupIDs="6ec3a0a8-a6c6-4cdf-a6e3-c296407a5ec1"
aadTenantID="3851f269-b22b-4de6-97d6-aa9fe60fe301"
objectId="890c52c5-d318-4185-a548-e07827190ff6"
spAppId=""
spPassword=""
logWorkspaceName="aks-train-lw"
lwResourceGroup="monitoring-workshop-rg"

#Login to Azure
az login --tenant $tenantId

#Check Selected Subscription
az account show

#Set appropriate Subscription, if needed
#az account set -s $subscriptionId

Pre-Config - Day 0
================================================================================
[Diagram - main]

#Create Service Principal
az ad sp create-for-rbac --skip-assignment -n https://aks-train-sp
{
  "appId": "be96b485-082f-406b-8271-06dcb442f6f2",
  "displayName": "https://aks-train-sp",
  "name": "be96b485-082f-406b-8271-06dcb442f6f2",
  "password": "ClHV~lb5n0MN.4WSiHNBiTnh2Ie5.mHR6_",
  "tenant": "72f988bf-86f1-41af-91ab-2d7cd011db47"
}

#Set Service Principal variables
spAppId="be96b485-082f-406b-8271-06dcb442f6f2"
spPassword="ClHV~lb5n0MN.4WSiHNBiTnh2Ie5.mHR6_"

#Create Master Resource Group for Hub workloads
az group create -n $masterResourceGroup -l $location

#View Resource Group for Hub workloads
az group show -g $masterResourceGroup

#Create Resource Group for AKS workloads
az group create -n $aksResourceGroup -l $location

#View Resource Group for AKS workloads
az group show -g $aksResourceGroup 

[Diagram - CNI]

#Deploy Hub Virtual Network
az network vnet create -n $masterVnetName -g $masterResourceGroup --address-prefixes $masterVnetPrefix
masterVnetId=$(az network vnet show -n $masterVnetName -g $masterResourceGroup --query="id" -o tsv)
echo $masterVnetId

#Deploy Jump Server Subnet inside Hub Virtual Network
az network vnet subnet create -n $masterSubnetName --vnet-name $masterVnetName -g $masterResourceGroup --address-prefixes $masterSubnetPrefix
masterSubnetId=$(az network vnet subnet show -n $masterSubnetName --vnet-name $masterVnetName -g $masterResourceGroup --query="id" -o tsv)
echo $masterSubnetId

#Deploy Spoke Virtual Network
az network vnet create -n $aksVnetName -g $aksResourceGroup --address-prefixes $aksVnetPrefix
aksVnetId=$(az network vnet show -n $aksVnetName -g $aksResourceGroup --query="id" -o tsv)
echo $aksVnetId

#Deploy AKS Subnet inside Spoke Virtual Network
az network vnet subnet create -n $aksSubnetName --vnet-name $aksVnetName -g $aksResourceGroup --address-prefixes $aksSubnetPrefix
aksSubnetId=$(az network vnet subnet show -n $aksSubnetName --vnet-name $aksVnetName -g $aksResourceGroup --query="id" -o tsv)
echo $aksSubnetId

az network nsg create -g $aksResourceGroup -n $aksVnetName-$aksSubnetName-$location

az network nsg rule create -n AllowCorpNet --nsg-name $aksVnetName-$aksSubnetName-$location -g $aksResourceGroup \
--access Allow --direction Inbound --source-address-prefixes "*" --source-port-ranges "*" \
--destination-address-prefixes "*" --destination-port-ranges "*" --priority 2700

az network nsg rule create -n AllowSAW --nsg-name $aksVnetName-$aksSubnetName-$location -g $aksResourceGroup \
--access Allow --direction Inbound --source-address-prefixes "*" --source-port-ranges "*" \
--destination-address-prefixes "*" --destination-port-ranges "*" --priority 2701

az network vnet subnet update --name $aksSubnetName --vnet-name $aksVnetName -g $aksResourceGroup \
--network-security-group $aksVnetName-$aksSubnetName-$location

#Deploy Ingress Subnet inside Spoke Virtual Network
az network vnet subnet create -n $aksIngressSubnetName --vnet-name $aksVnetName -g $aksResourceGroup --address-prefixes $aksIngressSubnetPrefix
aksIngressSubnetId=$(az network vnet subnet show -n $aksIngressSubnetName --vnet-name $aksVnetName -g $aksResourceGroup --query="id" -o tsv)
echo $aksIngressSubnetId

az network nsg create -g $aksResourceGroup -n $aksVnetName-$aksIngressSubnetName-$location

az network nsg rule create -n AllowCorpNet --nsg-name $aksVnetName-$aksIngressSubnetName-$location -g $aksResourceGroup \
--access Allow --direction Inbound --source-address-prefixes "*" --source-port-ranges "*" \
--destination-address-prefixes "*" --destination-port-ranges "*" --priority 2700

az network nsg rule create -n AllowSAW --nsg-name $aksVnetName-$aksIngressSubnetName-$location -g $aksResourceGroup \
--access Allow --direction Inbound --source-address-prefixes "*" --source-port-ranges "*" \
--destination-address-prefixes "*" --destination-port-ranges "*" --priority 2701

az network vnet subnet update --name $aksIngressSubnetName --vnet-name $aksVnetName -g $aksResourceGroup \
--network-security-group $aksVnetName-$aksIngressSubnetName-$location

#Deploy Application Gateway Subnet inside Spoke Virtual Network
az network vnet subnet create -n $appgwSubnetName --vnet-name $aksVnetName -g $aksResourceGroup --address-prefixes $appgwSubnetPrefix
appgwSubnetId=$(az network vnet subnet show -n $appgwSubnetName --vnet-name $aksVnetName -g $aksResourceGroup --query="id" -o tsv)
echo $appgwSubnetId

az network nsg create -g $aksResourceGroup -n $aksVnetName-$appgwSubnetName-$location

az network nsg rule create -n AllowCorpNet --nsg-name $aksVnetName-$appgwSubnetName-$location -g $aksResourceGroup \
--access Allow --direction Inbound --source-address-prefixes "*" --source-port-ranges "*" \
--destination-address-prefixes "*" --destination-port-ranges "*" --priority 2700

az network nsg rule create -n AllowSAW --nsg-name $aksVnetName-$appgwSubnetName-$location -g $aksResourceGroup \
--access Allow --direction Inbound --source-address-prefixes "*" --source-port-ranges "*" \
--destination-address-prefixes "*" --destination-port-ranges "*" --priority 2701

az network nsg rule create -n GatewayManger --nsg-name $aksVnetName-$appgwSubnetName-$location -g $aksResourceGroup \
--access Allow --direction Inbound --source-address-prefixes "*" --source-port-ranges "65200-65535" \
--destination-address-prefixes "*" --destination-port-ranges "*" --priority 2702

az network vnet subnet update --name $appgwSubnetName --vnet-name $aksVnetName -g $aksResourceGroup \
--network-security-group $aksVnetName-$appgwSubnetName-$location

#Deploy API Management Subnet inside Spoke Virtual Network
az network vnet subnet create -n $apimSubnetName --vnet-name $aksVnetName -g $aksResourceGroup --address-prefixes $apimSubnetPrefix
apimSubnetId=$(az network vnet subnet show -n $apimSubnetName --vnet-name $aksVnetName -g $aksResourceGroup --query="id" -o tsv)
echo $apimSubnetId

#Assign Role to Spoke Virtual Network
az role assignment create --assignee $spAppId --role "Network Contributor" --scope $aksVnetId

#Deploy ACR
az acr create -n $acrName -g $aksResourceGroup --sku STANDARD --admin-enabled false
acrId=$(az acr show -n $acrName -g $aksResourceGroup --query="id" -o tsv)
echo $acrId

#Assign Role to Service Principal for the ACR
az role assignment create --assignee $spAppId --role "AcrPull" --scope $acrId

#Deploy KeyVault
az keyvault create -n $keyVaultName -g $aksResourceGroup --sku Standard
objectId=$(az ad user show --id modatta@microsoft.com --query="objectId" -o tsv)
echo $objectId

#Set Access Policy to KeyVault for the loged in User 
az keyvault set-policy -n $keyVaultName -g $aksResourceGroup --key-permissions get list update create delete \
--secret-permissions get list set delete --certificate-permissions get list update create delete \
--object-id $objectId
keyvaultId=$(az keyvault show -n $keyVaultName -g $aksResourceGroup --query="id" -o tsv)
echo $keyvaultId

#Get LogAnalytics Workspace
logWorkspaceId=$(az monitor log-analytics workspace show -n $logWorkspaceName -g $lwResourceGroup --query="id" -o tsv)
echo $logWorkspaceId

#Assign Role to Service Principal for the LogAnalytics Workspace
az role assignment create --assignee $spAppId --role "Contributor" --scope $logWorkspaceId


================================================================================

Setup - Day 1
================================================================================

#Create Public AKS cluster
az aks create --name $clusterName \
--resource-group $aksResourceGroup \
--kubernetes-version $version --location $location \
--vnet-subnet-id "$aksSubnetId" --enable-addons $addons \
--service-cidr $aksServicePrefix --dns-service-ip $dnsServiceIP \
--node-vm-size $sysPoolNodeSize \
--node-count $sysPoolNodeCount --max-pods $sysPoolMaxPods \
--service-principal $spAppId \
--client-secret $spPassword \
--network-plugin $networkPlugin --network-policy $networkPolicy \
--nodepool-name $sysPoolName --vm-set-type $vmSetType \
--generate-ssh-keys \
--enable-aad \
--aad-admin-group-object-ids $aadAdminGroupIDs \
--aad-tenant-id $aadTenantID \
--attach-acr $acrName --workspace-resource-id $logWorkspaceId

#Connect to AKS cluster and check status
az aks get-credentials -g $aksResourceGroup --name $clusterName --admin --overwrite
kubectl get no
kubectl get ns

#Update System nodepool with AutoScaling
az aks nodepool update --cluster-name $clusterName --resource-group $aksResourceGroup \
--enable-cluster-autoscaler --min-count $sysPoolNodeCount --max-count $sysPoolMaxNodeCount \
--name $sysPoolName

#Create Additional Nodepool - API Nodepool
az aks nodepool add --cluster-name $clusterName --resource-group $aksResourceGroup \
--name $apiPoolName --kubernetes-version $version --max-pods $apiPoolMaxPods \
--node-count $apiPoolNodeCount --node-vm-size $apiPoolNodeSize --os-type $osType \
--mode User

#Update API Nsodepool with AutoScaling
az aks nodepool update --cluster-name $clusterName --resource-group $aksResourceGroup \
--enable-cluster-autoscaler --min-count $apiPoolNodeCount --max-count $apiPoolMaxNodeCount \
--name $apiPoolName

================================================================================

Post-Config - Day 2
================================================================================

#Secure AKS cluster

[Diagram - aks+appgw]

#Choose a Static Private IP from $aksIngressSubnetName
backendIpAddress="18.0.1.100"

[Diagram - DNS - TBD]

#A Private DNS Zone is needed to resolve all Private IP addresses
#Prepare Azure Private DNS Zone

#Create Azure Private DNS Zone
az network private-dns zone create -n $privateDNSZoneName -g $masterResourceGroup
privateDNSZoneId=$(az network private-dns zone show -g $masterResourceGroup -n $privateDNSZoneName --query="id" -o tsv)
echo $privateDNSZoneId

#Add RecordSet for dev
az network private-dns record-set a create -n aks-train-dev -g $masterResourceGroup --zone-name $privateDNSZoneName
az network private-dns record-set a add-record -a $backendIpAddress -n aks-train-dev -g $masterResourceGroup -z $privateDNSZoneName

#Add RecordSet for qa
az network private-dns record-set a create -n aks-train-qa -g $masterResourceGroup --zone-name $privateDNSZoneName
az network private-dns record-set a add-record -a $backendIpAddress -n aks-train-qa -g $masterResourceGroup -z $privateDNSZoneName

#Add RecordSet for smoke
az network private-dns record-set a create -n aks-train-smoke -g $masterResourceGroup --zone-name $privateDNSZoneName
az network private-dns record-set a add-record -a $backendIpAddress -n aks-train-smoke -g $masterResourceGroup -z $privateDNSZoneName

#Link master Virtual Network to Private DNS Zone
az network private-dns link vnet create -g $masterResourceGroup -n $masterPrivateDNSLink -z $privateDNSZoneName -v $masterVnetId -e false
az network private-dns link vnet show -g $masterResourceGroup -n $masterPrivateDNSLink -z $privateDNSZoneName

#Link AKS Virtual Network to Private DNS Zone
az network private-dns link vnet create -g $masterResourceGroup -n $aksPrivateDNSLink -z $privateDNSZoneName -v $aksVnetId -e false
az network private-dns link vnet show -g $masterResourceGroup -n $aksPrivateDNSLink -z $privateDNSZoneName

#Create Ingress Namespace
kubectl create namespace $aksIngControllerNSName
#kubectl label namespace $aksIngControllerNSName name=$aksIngControllerNSName

#Install nginx as ILB using Helm
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

#Install Ingress controller
helm install $aksIngControllerName ingress-nginx/ingress-nginx --namespace $aksIngControllerNSName \

#Specify configuration values for Ingress controller 
-f $ingControllerFilePath \

#Ensures Private IP for Nginx Ingress Controller
--set controller.service.loadBalancerIP=$backendIpAddress \

#Ensures that the Nginx Ingress Controller is deployed only on System pool (Good practice)
--set controller.nodeSelector.agentpool=$sysPoolName \
--set controller.defaultBackend.nodeSelector.agentpool=$sysPoolName \

#Specify the Subnet from Ingress controller should pick IP addresses (Good practice)
--set controller.service.annotations.'service\.beta\.kubernetes\.io/azure-load-balancer-internal-subnet'=$aksIngressSubnetName

helm install $aksIngControllerName ingress-nginx/ingress-nginx --namespace $aksIngControllerNSName \
-f $aksIngControllerFilePath \
--set controller.service.loadBalancerIP=$backendIpAddress \
--set controller.nodeSelector.agentpool=$sysPoolName \
--set controller.defaultBackend.nodeSelector.agentpool=$sysPoolName \
--set controller.service.annotations.'service\.beta\.kubernetes\.io/azure-load-balancer-internal-subnet'=$aksIngressSubnetName

#helm uninstall $aksIngControllerName --namespace $aksIngControllerNSName

#Check Ingress Controller IP
kubectl get svc -A

#Create Namespaces
#DEV workloads
kubectl create ns aks-train-dev

#QA workloads
kubectl create ns aks-train-qa

#Smoke Test
kubectl create ns aks-train-smoke

/*
#Deploy Application Gateway
az deployment group create -f ./aksauto-appgw-deploy.bicep -g $aksResourceGroup \
--parameters @./aksauto-appgw-deploy.parameters.json \
--parameters applicationGatewayName=$appgwName \
vnetName=$aksVnetName subnetName=$appgwSubnetName \
httpsListenerNames=$httpsListenerNames \
backendIpAddress=$backendIpAddress
*/

Application Gateway
=====================
[Diagram - appgw - multiple]

#Create Application Gateway from Azure Portal
<Steps from endtoendSSL.md>
#backend pool of app gateway points to Private IP of Nginx Ingress Controller

RBAC
================================================================================
#Deploy RBAC for the AKS cluster
helm create rbac-chart

helm install rbac-chart -n aks-train-dev $setupFolderPath/Helms/rbac-chart/ -f $setupFolderPath/Helms/rbac-chart/values-dev.yaml
#helm upgrade rbac-chart -n aks-train-dev $setupFolderPath/Helms/rbac-chart/ -f $setupFolderPath/Helms/rbac-chart/values-dev.yaml

helm install rbac-chart -n aks-train-qa $setupFolderPath/Helms/rbac-chart/ -f $setupFolderPath/Helms/rbac-chart/values-qa.yaml
#helm upgrade rbac-chart -n aks-train-qa $setupFolderPath/Helms/rbac-chart/ -f $setupFolderPath/Helms/rbac-chart/values-qa.yaml

#helm uninstall rbac-chart

#Check access by multiple login ids
az aks get-credentials -g $aksResourceGroup --name $clusterName
kubectl get no
kubectl get ns

================================================================================

Ingress - Smoke
================================================================================
#Deploy Ingress Rule object for Smoke namespace
helm create ingress-chart

helm install ingress-chart -n aks-train-smoke $setupFolderPath/Helms/ingress-chart/ -f $setupFolderPath/Helms/ingress-chart/values-smoke.yaml

#helm upgrade  ingress-chart -n aks-train-smoke $setupFolderPath/Helms/ingress-chart/ -f $setupFolderPath/Helms/ingress-chart/values-smoke.yaml
#helm uninstall ingress-chart -n aks-train-smoke

================================================================================

TEST - Smoke
================================================================================
#Test Cluster Health and end-to-end connectivity
#Deploy Nginx app in Smoke Namespace

az acr import -n $acrName --source docker.io/library/nginx:alpine -t nginx:alpine

helm create smoke-tests-chart

helm install smoke-tests-chart -n aks-train-smoke $setupFolderPath/Helms/smoke-tests-chart/ -f $setupFolderPath/Helms/smoke-tests-chart/values-smoke.yaml

#helm upgrade smoke-tests-chart -n aks-train-smoke $setupFolderPath/Helms/smoke-tests-chart/ -f $setupFolderPath/Helms/smoke-tests-chart/values-smoke.yaml
#helm uninstall smoke-tests-chart -n aks-train-smoke

#Call Nginx app Url; check end-to-end connectivity
curl -k https://smoke-<appgw-dns-name>/healthz

================================================================================

Ratings App - DEV
=====================
[Diagram - RatingsApp]
#Deploy more apps - Ratings app

#Deploy backend Mongo DB as container
kubectl create ns db

helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

helm install ratingsdb bitnami/mongodb -n db \
--set auth.username=ratingsuser,auth.password=ratingspwd,auth.database=ratingsdb \
--set controller.nodeSelector.agentpool=$sysPoolName \
--set controller.defaultBackend.nodeSelector.agentpool=$sysPoolName

#Remove backend Mongo DB container
#helm uninstall ratingsdb

#RatingsApi - Ratings API backend 

#Clone/Fork/Download Souerce code
https://github.com/monojit18/mslearn-aks-workshop-ratings-api.git

#CD to the director where Dockerfile exists
#This docker build but performed in a Cloud Agent(VM) by ACR
az acr build -t $acrName.azurecr.io/ratings-api:v1.0.0 -r $acrName .

kubectl create secret generic aks-workshop-mongo-secret -n aks-train-dev \
--from-literal=MONGOCONNECTION="mongodb://ratingsuser:ratingspwd@ratingsdb-mongodb.db:27017/ratingsdb"

#Change <acrName> in the $microservicesFolderPath/Helms/ratingsapi-chart/values-dev.yaml
#Change <agentpool> in the $microservicesFolderPath/Helms/ratingsapi-chart/values-dev.yaml
helm install ratingsapi-chart -n aks-train-dev $microservicesFolderPath/Helms/ratingsapi-chart/ -f $microservicesFolderPath/Helms/ratingsapi-chart/values-dev.yaml

#helm upgrade ratingsapi-chart -n aks-train-dev $microservicesFolderPath/Helms/ratingsapi-chart/ -f $microservicesFolderPath/Helms/ratingsapi-chart/values-dev.yaml
#helm uninstall ratingsapi-chart -n aks-train-dev

#RatingsWeb - Ratings App Frontend
#Clone/Fork/Download Souerce code
https://github.com/monojit18/mslearn-aks-workshop-ratings-web.git

#CD to the director where Dockerfile exists
#This docker build but performed in a Cloud Agent(VM) by ACR
az acr build -t $acrName.azurecr.io/ratings-web:v1.0.0 -r $acrName .

#Change <acrName> in the $microservicesFolderPath/Helms/ratingsapi-chart/values-dev.yaml
#Change <agentpool> in the $microservicesFolderPath/Helms/ratingsapi-chart/values-dev.yaml
helm install ratingsweb-chart -n aks-train-dev $microservicesFolderPath/Helms/ratingsweb-chart/ -f $microservicesFolderPath/Helms/ratingsweb-chart/values-dev.yaml

#helm upgrade ratingsweb-chart -n aks-train-dev $microservicesFolderPath/Helms/ratingsweb-chart/ -f $microservicesFolderPath/Helms/ratingsweb-chart/values-dev.yaml
#helm uninstall ratingsweb-chart -n aks-train-dev

Ingress - DEV
=====================
#Deploy Ingress Rule object for DEV namespace
helm create ingress-chart

helm install  ingress-chart -n aks-train-dev $setupFolderPath/Helms/ingress-chart/ -f $setupFolderPath/Helms/ingress-chart/values-dev.yaml

#helm upgrade  ingress-chart -n aks-train-dev $setupFolderPath/Helms/ingress-chart/ -f $setupFolderPath/Helms/ingress-chart/values-dev.yaml
#helm uninstall ingress-chart -n aks-train-dev

#Call Ratings app Url; check end-to-end connectivity
curl -k https://dev-<appgw-dns-name>/

================================================================================

Ratings App - QA
=====================
#Deploy more apps - Ratings app

kubectl create secret generic aks-workshop-mongo-secret -n aks-train-qa --context=$CTX_CLUSTER1 \
--from-literal=MONGOCONNECTION="mongodb://ratingsuser:ratingspwd@ratingsdb-mongodb.db:27017/ratingsdb"

#Change <acrName> in the $microservicesFolderPath/Helms/ratingsapi-chart/values-qa.yaml
#Change <agentpool> in the $microservicesFolderPath/Helms/ratingsapi-chart/values-qa.yaml
helm install ratingsapi-chart -n aks-train-qa $microservicesFolderPath/Helms/ratingsapi-chart/ -f $microservicesFolderPath/Helms/ratingsapi-chart/values-qa.yaml

#helm upgrade ratingsapi-chart -n aks-train-qa $microservicesFolderPath/Helms/ratingsapi-chart/ -f $microservicesFolderPath/Helms/ratingsapi-chart/values-qa.yaml
#helm uninstall ratingsapi-chart -n aks-train-qa

#RatingsWeb - Ratings App Frontend
#Change <acrName> in the $microservicesFolderPath/Helms/ratingsapi-chart/values-qa.yaml
#Change <agentpool> in the $microservicesFolderPath/Helms/ratingsapi-chart/values-qa.yaml
helm install ratingsweb-chart -n aks-train-qa $microservicesFolderPath/Helms/ratingsweb-chart/ -f $microservicesFolderPath/Helms/ratingsweb-chart/values-qa.yaml

#helm upgrade ratingsweb-chart -n aks-train-qa $microservicesFolderPath/Helms/ratingsweb-chart/ -f $microservicesFolderPath/Helms/ratingsweb-chart/values-qa.yaml
#helm uninstall ratingsweb-chart -n aks-train-qa

Ingress - QA
=====================
#Deploy Ingress Rule object for QA namespace

helm install  ingress-chart -n aks-train-qa $setupFolderPath/Helms/ingress-chart/ -f $setupFolderPath/Helms/ingress-chart/values-qa.yaml

#helm upgrade  ingress-chart -n aks-train-qa $setupFolderPath/Helms/ingress-chart/ -f $setupFolderPath/Helms/ingress-chart/values-qa.yaml
#helm uninstall ingress-chart -n aks-train-qa

#Call Ratings app Url; check end-to-end connectivity
curl -k https://qa-<appgw-dns-name>/

================================================================================

Network Policies - DEV
=========================
[Diagram - Network Policy]

#East-West Traffic Security
helm install netpol-chart -n aks-train-dev $setupFolderPath/Helms/netpol-chart/ -f $setupFolderPath/Helms/netpol-chart/values-dev.yaml

#helm upgrade netpol-chart -n aks-train-dev $setupFolderPath/Helms/netpol-chart/ -f $setupFolderPath/Helms/netpol-chart/values-dev.yaml
#helm uninstall netpol-chart -n aks-train-dev

Network Policies - QA
=========================
helm install netpol-chart -n aks-train-qa $setupFolderPath/Helms/netpol-chart/ -f $setupFolderPath/Helms/netpol-chart/values-qa.yaml

#helm upgrade netpol-chart -n aks-train-qa $setupFolderPath/Helms/netpol-chart/ -f $setupFolderPath/Helms/netpol-chart/values-qa.yaml
#helm uninstall netpol-chart -n aks-train-qa

Network Policies - Smoke
=========================
helm install netpol-chart -n aks-train-smoke $setupFolderPath/Helms/netpol-chart/ -f $setupFolderPath/Helms/netpol-chart/values-smoke.yaml

#helm upgrade netpol-chart -n aks-train-smoke $setupFolderPath/Helms/netpol-chart/ -f $setupFolderPath/Helms/netpol-chart/values-smoke.yaml
#helm uninstall netpol-chart -n aks-train-smoke

#Call Ratings app Url; check end-to-end connectivity
curl -k https://dev-<appgw-dns-name>/
curl -k https://qa-<appgw-dns-name>/
curl -k https://smoke-<appgw-dns-name>/nginx

podName=$(kubectl get pod -l app=nginx-pod -n primary -o jsonpath='{.items[0].metadata.name}')

#Should succeed
kubectl exec -it $podName -n aks-train-smoke -- curl -k http://ratingsapp-web.aks-train-dev.svc/

#Should succeed
kubectl exec -it $podName -n aks-train-smoke -- curl -k http://ratingsapp-web.aks-train-qa.svc/

podName=$(kubectl get pod -l app=ratingsweb-pod -n primary -o jsonpath='{.items[0].metadata.name}')

#Should FAIL
kubectl exec -it $podName -n aks-train-dev -- curl -k http://ratingsapp-web.aks-train-qa.svc/

#Should FAIL
kubectl exec -it $podName -n aks-train-dev -- curl -k http://nginx-svc.aks-train-smoke.svc/

================================================================================
Cluster Health
Node and Pod Health
Observe and Analyze Workload Deployments
  #View Metrics from Azure Portal
  #View Insights from Azure Portal
  #Create a Dashboard in Azure Portal
  #Log Analytics with Container Insights
  #Select Pre-defined Queries and Check Results
  #Create Azure Monitor Workbook and View Results

[Multiple Screenshots]


#Enable Prometheus for AKS
#Azure Monitor with Prometheus
https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-prometheus-integration#configure-and-deploy-configmaps

#Prometheus config map
https://aka.ms/container-azm-ms-agentconfig

=========================================================

[Multiple Screenshots]

#AKS Monitoring with Grafana
https://github.com/grafana/helm-charts/blob/main/charts/grafana/README.md

helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

kubectl create ns grafana-monitor
helm install aks-train-grafana -n grafana-monitor grafana/grafana --set nodeSelector.agentpool=$sysPoolName
#helm uninstall aks-train-grafana -n grafana-monitor


#Integrate Grafan with Azure Monitor
https://grafana.com/grafana/plugins/grafana-azure-monitor-datasource/


================================================================================
#Load testing with JMeter
https://techcommunity.microsoft.com/t5/azure-global/scalable-apache-jmeter-test-framework-using-azure-kubernetes/ba-p/1197379


#UNCOMMENT: HPA in .helmignore for RatingsApi app
#Redeploy RatingsApi app
#Open JMeter

================================================================================

Cluster Upgrade
================================================================================
[Diagram - TBD]

Refer Here: https://docs.microsoft.com/en-us/azure/aks/upgrade-cluster

#Max surge
How Many Additional Nodes to add while Upgrade in progress?
Default = 1
Standard/Optial = 33% (of existing no. of nodes in Nodepool)
This value can be Integer as well as Percentage

During an upgrade
- Minimum of Max Surge can be 1
- Maximum of Max Surge can be equal to the number of nodes in your node pool
Larger values can be set but the maximum number of nodes used for max surge won't be higher than the number of nodes in the pool at the time of upgrade.

#Steps

- Add a new buffer node (or as many nodes as configured in max surge) to the cluster that runs the specified Kubernetes version

- Cordon and drain one of the old nodes to minimize disruption to running applications (if you're using max surge it will cordon and drain as many nodes at the same time as the number of buffer nodes specified).

- When the old node is fully drained, it will be reimaged to receive the new version and it will become the buffer node for the following node to be upgraded

- This process repeats until all nodes in the cluster have been upgraded

- At the end of the process, the last buffer node will be deleted, maintaining the existing agent node count and zone balance

upgradeVersion=""

az aks get-upgrades --resource-group $aksResourceGroup --name $clusterName --output table
az aks upgrade --resource-group $aksResourceGroup --name $clusterName --kubernetes-version $upgradeVersion

#Check if Upgrade is successful
az aks show --resource-group $aksResourceGroup --name $clusterName --output table

================================================================================

Node Image Upgrade
================================================================================
#Check Node details
kubectl describe nodes <NodeName>

#Update All Nodes in All Nodepools
az aks upgrade --resource-group $aksResourceGroup --name $clusterName --node-image-only

#Update All Nodes in a specific Nodepool (Not recommened!!)
$nodepoolToUpgrade=""
az aks upgrade --resource-group $aksResourceGroup --name $clusterName --node-image-only
az aks nodepool upgrade --resource-group $aksResourceGroup --name $clusterName --name $nodepoolToUpgrade --node-image-only

#Upgrade with additional Nodes to avoid any downtime
az aks nodepool upgrade --resource-group $aksResourceGroup --name $clusterName --name $nodepoolToUpgrade \
    --max-surge 33% --node-image-only --no-wait


================================================================================


#Cleanup resources - Individual
================================================================================
#az aks delete -g $aksResourceGroup -n $clusterName --yes
#az acr delete -g $aksResourceGroup -n $acrName --yes
#az keyvault delete -g $aksResourceGroup -n $keyVaultName --yes
#az network application-gateway delete -g $aksResourceGroup -n $aksVnetName --yes
#az network vnet delete -g $aksResourceGroup -n $aksVnetName --yes

#Cleanup resources - All
================================================================================
#az group delete -n $aksResourceGroup --yes

================================================================================================================================================================



AKS - ADVANCED TO PRO
================================================================================================================================================================
#Pre-Requisites
================================================================================
[Diagram - Main]

Gone thru the exercises of Basic to Advanced

AKS cluster already created

3 Namespaces created
- aks-train-dev
- aks-train-qa
- aks-train-smoke

Applications Deployed
- Ratings App (DEV and QA)
- Nginx App (Smoke)

Nginx Ingress Controller deployed; with Private IP

API routing is happening thru K8s Ingress rules

Network Policies deployed for East-West traffic
- Successfully Tested with various configurations

Monitoring enabled
- Azure Monitor
- Log Analytics
- Grafana (Integrated with Azure Monitor)

End to End connectivity established
- Application Gateway as Public facing L7 LoadBalancer
- Multi-Tenancy implemnented tru Application Gateway Listeners and Http Settings

Load Testing
- JMeter Local
- JMX files created for RatingsApi app 
- HPA for RatingsApi enabled
- Successfully Tested with Load

================================================================================
#Local Variables

baseFolderPath=""
setupFolderPath="$baseFolderPath/Setup"
microservicesFolderPath="$baseFolderPath/Microservices"
tenantId=""
subscriptionId=""
aksResourceGroup=""
masterResourceGroup=""
location=""
clusterName=""
version=""
acrName=""
acrId=
keyVaultName=""
keyvaultId=
objectId=
masterVnetName=""
masterVnetPrefix=""
masterVnetId=
masterSubnetName=""
masterSubnetPrefix=""
masterSubnetId=
aksVnetName=""
aksVnetPrefix=""
aksVnetId=
aksSubnetName=""
aksSubnetPrefix=""
aksSubnetId=
aksIngressSubnetName=""
aksIngressSubnetPrefix=""
aksIngressSubnetId=
appgwName=""
appgwSubnetName=""
appgwSubnetPrefix=""
appgwSubnetId=
apimName=""
apimSubnetName=""
apimSubnetPrefix=""
apimSubnetId=
sysPoolName=akssyspool
sysPoolNodeSize="Standard_DS3_v2"
sysPoolNodeCount=3
sysPoolMaxPods=30
sysPoolMaxNodeCount=5
apiPoolName=aksapipool
apiPoolNodeSize="Standard_DS3_v2"
apiPoolNodeCount=3
apiPoolMaxPods=30
apiPoolMaxNodeCount=10
networkPlugin=azure
networkPolicy=azure
vmSetType=VirtualMachineScaleSets
addons=monitoring
aadAdminGroupID=""
aadTenantID=""
spAppId=""
spPassword=""
masterAKSPeering="$masterVnetName-$aksVnetName-peering"
aksMasterPeering="$aksVnetName-$masterVnetName-peering"
masterPrivateDNSLink="$masterVnetName-aks-dns-link"
aksPrivateDNSLink="$aksVnetName-dns-link"
aksIngControllerName=""
aksIngControllerNSName=""
aksIngControllerFileName="internal-ingress"
privateDNSZoneName=""
privateDNSZoneId=
httpsListenerNames='("dev","qa")'
backendIpAddress=
aadAdminGroupIDs='("")'
aadTenantID=""

#Login to Azure
az login --tenant $tenantId

#Check Selected Subscription
az account show

#Set appropriate Subscription, if needed
#az account set -s $subscriptionId

API Management
================================================================================
[diagram - AKS+APIM]

#API Management deployment
Install API Management through Azure Portal

Move APIM into a Subnet (already created in previous exercise)
- A private IP is assigned to APIM instance

A Custom Domian for APIM is configured
- Using a proper DNS certificate
- LetsEncrypt Certificates can also be used

Modify Application Gateway backend pool to point to this Private IP
(Earlier it was pointing to Private IP of Nginx Ingress)

Modify Http Settings of Application Gateway to point all traffic to APIM

Modify Health Probe accordingly

Define APIs behind APIM

Check end-to-end connectivity

Configure OAuth for APIM
- Configure APIs with OAuth definition
- Add Policies for JWT header validation
- Authenticate Each API
- Generate Bearer Token by making a call to Azure AD Graph API (POSTMAN or any REST client can be used)
- Pass this Token with the Authorization header of each API call
- APIM policies should allow/reject API calls accordingly

Check end-to-end connectivity

Enable Application Insights in APIM

Check Metrics in Azure Portal

================================================================================

KEDA
================================================================================
[Diagram - KEDA Arch]

#Kubernetes based Event Driven AutoScaling - Severless, Event Driven Apps

Refer Here - https://keda.sh/docs/2.4/deploy/

#Add Helm repo
helm repo add kedacore https://kedacore.github.io/charts

#Update Helm repo
helm repo update

#Install keda Helm chart
kubectl create namespace keda
helm install keda kedacore/keda --namespace keda

#Create Namespace for Serverless apps
kubectl create ns serverless

#Create a Storage Account in Azure Portal - kedateststg
#Create a Blob Container in Azure Portal - kedablob
#Create a Queue Container in Azure Portal - kedaqueue

#Note down the Connection String of the storage account
#This would be added as K8s secret inside the AKS cluster
kubetcl create secret generic keda-stg-secret -n serverless --from-literal=AzureWebJobsStorage="<Blob-Connection-String>"

ACIBlobApp
================================================================================
#Deploy ACIBlobApp in serverless namespace
#App reacts to Blob events
helm install aciblobapp-chart -n serverless $microservicesFolderPath/Helms/aciblobapp-chart/ -f $microservicesFolderPath/Helms/aciblobapp-chart/AKSWorkshop/values-dev.yaml

#helm upgrade aciblobapp-chart -n serverless $microservicesFolderPath/Helms/aciblobapp-chart/ -f $microservicesFolderPath/Helms/aciblobapp-chart/AKSWorkshop/values-dev.yaml
#helm uninstall aciblobapp-chart -n serverless

#Check if app is deployed and no. of replicas running
kubectl get all -n serverless

#Modify the values-dev.yaml file to update placeholder values

#Deploy KEDA objects in serverless namespace
#These objects would ensure that the application scales based on Blob trigger
helm install aciblobapp-chart -n serverless $microservicesFolderPath/Helms/aciblobapp-keda-chart/ -f $microservicesFolderPath/Helms/aciblobapp-keda-chart/AKSWorkshop/values-dev.yaml

#helm upgrade aciblobapp-chart -n serverless $microservicesFolderPath/Helms/aciblobapp-keda-chart/ -f $microservicesFolderPath/Helms/aciblobapp-keda-chart/AKSWorkshop/values-dev.yaml
#helm uninstall aciblobapp-chart -n serverless

#Keep adding large no. of images into Blob Container
#Check ACIBlobApp deployment on AKS cluster
#Watch how replicas are scaling up and down
================================================================================

ACIQueueApp
================================================================================
#Deploy ACIQueueApp in serverless namespace
#App reacts to Blob events
helm install aciqueueapp-chart -n serverless $microservicesFolderPath/Helms/aciqueueapp-chart/ -f $microservicesFolderPath/Helms/aciqueueapp-chart/AKSWorkshop/values-dev.yaml

#helm upgrade aciqueueapp-chart -n serverless $microservicesFolderPath/Helms/aciqueueapp-chart/ -f $microservicesFolderPath/Helms/aciqueueapp-chart/AKSWorkshop/values-dev.yaml
#helm uninstall aciqueueapp-chart -n serverless

#Check if app is deployed and no. of replicas running
kubectl get all -n serverless

#Modify the values-dev.yaml file to update placeholder values

#Deploy KEDA objects in serverless namespace
#These objects would ensure that the application scales based on Blob trigger
helm install aciqueueapp-chart -n serverless $microservicesFolderPath/Helms/aciqueueapp-keda-chart/ -f $microservicesFolderPath/Helms/aciqueueapp-keda-chart/AKSWorkshop/values-dev.yaml

#helm upgrade aciblobapp-chart -n serverless $microservicesFolderPath/Helms/aciqueueapp-keda-chart/ -f $microservicesFolderPath/Helms/aciqueueapp-keda-chart/AKSWorkshop/values-dev.yaml
#helm uninstall aciqueueapp-chart -n serverless

#Keep adding large no. of messages into Queue Container
#Check ACIQueueApp deployment on AKS cluster
#Watch how replicas are scaling up and down

================================================================================

#Uninstall KEDA
#helm uninstall keda -n keda

================================================================================

Azure Policy
================================================================================
Refer Here:
https://docs.microsoft.com/en-us/azure/aks/policy-reference
https://docs.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes


================================================================================

Service Mesh
================================================================================
[From ServiceMeshWorkshop.md]

#Service Mesh deployment

What it is?

Why?

Features
- Observability
- Distributed Tracing
- Traffic Splitting
- Blue/Green deployment
- Fault Injection
- Circuit Breaking
- Multi Cluster
  - Traffic Mirroring or Shadowing
  - Multi Cluster Connecitivity

Remove Nginx Ingress Controller
- The in-built Ingress Gateway from Service mesh would be used

#Set CLI Variables for Istio
================================================================================
primaryResourceGroup=$aksResourceGroup
primaryClusterName="primary-mesh-cluster"
secondaryResourceGroup="secondary-workshop-rg"
secondaryClusterName="secondary-mesh-cluster"
primaryAcrName=$acrName
istioPath="$microservicesFolderPath/Istio"

#Set Env Variable for Primary Cluster
#This helps to switch context easily between multiple clusters
export CTX_CLUSTER1=primary

#Connect to Public AKS Cluster with Primary Context
az aks get-credentials -g $primaryResourceGroup -n $primaryClusterName --context $CTX_CLUSTER1

================================================================================

Download Istio
================================================================================
#Download Istio binary
curl -L https://istio.io/downloadIstio | sh -

#Download specific version of Istio viz. 1.11.3
curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.11.3 TARGET_ARCH=x86_64 sh -

#The istioctl client binary in the bin/ directory
#Add the istioctl client to your path (Linux or macOS):
export PATH=$PWD/bin:$PATH

================================================================================

Install and Configure Istio
================================================================================
#Create namespaces for Istio
kubectl create namespace istio-system --context $CTX_CLUSTER1
kubectl create namespace primary --context $CTX_CLUSTER1

#Install Istio CLI
#Select Default Istio Profile settings
#Ingress Gateway with Public IP Address
istioctl install --context=$CTX_CLUSTER1 --set profile=default -y

#Install Istio with custom configurations
#Ingress Gateway with Privae IP Address
#Another Publicly exposed LoadBalancer Service(L7) would be needed to access this Private IP
istioctl install --context=$CTX_CLUSTER1 -f $istioPath/Components/values-primary.yaml -y

#Inject Istio into primary namespace
#Ensures sidecar container to be added for every deployment in this namespace
kubectl label namespace primary istio-injection=enabled --context=$CTX_CLUSTER1

#Disable sidecar injection from primary namespace
#kubectl label namespace primary istio-injection=disabled --context=$CTX_CLUSTER1

#Install Istio Addons
#This primarily installs all dependencies for observability by Istio viz. Grafana, Kiali dashboard etc.
kubectl apply -f $istioPath/Components/samples/addons --context=$CTX_CLUSTER1

#Check rollout status of the Kiali deployment - usually takes sometime
kubectl rollout status deployment/kiali -n istio-system

#Check Deployments within istio-system
#Istio Ingress gateway with public or private IP
kubectl get svc -n istio-system

#Need a Gateway to expose the Kiali service outside
#Check Routing definitions
#Replace <dns-name>
kubectl apply -f $istioPath/Examples/Networking/kiali-gateway.yaml -n istio-system --context=$CTX_CLUSTER1

#Modify Application Gateway to accommodate Kiali Gateway to access Kiali Services
Configure Private DNS Zone for Kiali Gateway with Istio Ingress Gateway IP
Add Listener for Kiali Gateway
Add Http Settings for Kiali Gateway
Add Rules for Kiali Gateway Listener

#Launch Kiali in the browser
curl -k https://kiali-<appgw-dns-name>/

================================================================================

Observability
================================================================================
#Deploy Apps to view in Istio

#Install BookInfo app onto the cluster
kubectl apply -f $istioPath/Examples/BookInfo/bookinfo.yaml -n primary --context=$CTX_CLUSTER1

#Check Deployed Components
kubectl get svc -n primary --context=$CTX_CLUSTER1
kubectl get pods -n primary --context=$CTX_CLUSTER1

#Quick check to test BookInfo app
podName=$(kubectl get pod -l app=ratings -n primary -o jsonpath='{.items[0].metadata.name}')
kubectl exec $podName -n primary -c ratings -- curl -sS productpage:9080/productpage | grep -o "<title>.*</title>"

#Need a Gateway to expose the service outside
#Check Routing definitions
#Replace <dns-name>
kubectl apply -f $istioPath/Examples/Networking/primary-gateway.yaml -n primary --context=$CTX_CLUSTER1

#Get GATEWAY_URL
kubectl get svc istio-ingressgateway -n istio-system
export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')

#Call services using GATEWAY_URL
export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
echo "$GATEWAY_URL"

#Modify Application Gateway to accommodate RatingsApp Gateway to access RatingsApp Services
Configure Private DNS Zone for RatingsApp Gateway with Istio Ingress Gateway IP
Add Listener for RatingsApp Gateway
Add Http Settings for RatingsApp Gateway
Add Rules for RatingsApp Gateway Listener

#Try the follwoing URL in the Browser or do a cUrl
curl -k https://ratings-<appgw-dns-name>/

================================================================================

Traffic Shifting
================================================================================
#Traffic Shifting
kubectl apply -f $istioPath/Examples/HelloWorld/helloworld-app.yaml -n primary --context=$CTX_CLUSTER1
kubectl get po -n primary --context=$CTX_CLUSTER1

#Check Routing definitions
kubectl apply -f $istioPath/Examples/Networking/primary-gateway.yaml -n primary --context=$CTX_CLUSTER1

kubectl apply -f $istioPath/Examples/HelloWorld/helloworld-app-v2.yaml -n primary --context=$CTX_CLUSTER1
kubectl get po -n primary --context=$CTX_CLUSTER1

#Check Routing behaviour
#UNCOMMENT: Test Traffic Shifting
#Update Primary Gateway Routes - Change Traffic weight
kubectl apply -f $istioPath/Examples/Networking/primary-gateway.yaml -n primary --context=$CTX_CLUSTER1

#Destination Rule
kubectl apply -f $istioPath/Examples/Networking/helloworld-destination-rule.yaml -n primary --context=$CTX_CLUSTER1

#Check Routing behaviour again

=====================================================================================

Blue/Green
================================================================================
#Blue/Green
#Deploy PodInfo Blue
kubectl apply -f $istioPath/Examples/BlueGreen/podinfo-blue.yaml -n primary --context=$CTX_CLUSTER1

#Check Routing definitions
kubectl apply -f $istioPath/Examples/Networking/primary-gateway.yaml -n primary --context=$CTX_CLUSTER1

#Destination Rule
kubectl apply -f $istioPath/Examples/Networking/podinfo-destination-rule.yaml -n primary --context=$CTX_CLUSTER1

#Deploy PodInfo green
kubectl apply -f $istioPath/Examples/BlueGreen/podinfo-green.yaml -n primary --context=$CTX_CLUSTER1

#Check Routing behaviour
#UNCOMMENT: Test Blue/Green
#Update Primary Gateway Routes - Change Traffic weight
kubectl apply -f $istioPath/Examples/Networking/primary-gateway.yaml -n primary --context=$CTX_CLUSTER1

#Destination Rule
kubectl apply -f $istioPath/Examples/Networking/podinfo-destination-rule.yaml -n primary --context=$CTX_CLUSTER1

#Check Routing behaviour again

=============================================================================

Fault Injection
================================================================================
#Fault Injection
#Deploy Fault in Ratinsg API
kubectl apply -f $istioPath/Examples/Network/ratingsfault-virtual-service.yaml -n primary --context=$CTX_CLUSTER1

#Check Comments in the file
#Introduce Delay
#Check Routing behaviour

#Introduce Fix
kubectl apply -f $istioPath/Examples/Networking/reviewsfix-virtual-service.yaml -n primary --context=$CTX_CLUSTER1

#Check Comments in the file
#Check Routing behaviour

=============================================================================

Circuit Breaker
================================================================================
#Circuit Breaker
#Deploy HttpBin App
kubectl apply -f $istioPath/Examples/HttpBin/httpbin.yaml -n primary --context=$CTX_CLUSTER1
kubectl apply -f $istioPath/Examples/Networking/httpbin-gateway.yaml -n primary --context=$CTX_CLUSTER1

#Deploy HttpBin Destination Rule
kubectl apply -f $istioPath/Examples/Networking/httpbin-destination-rule.yaml -n primary --context=$CTX_CLUSTER1

#Check Routing behaviour
#UNCOMMENT: Test Circuit Breaking
kubectl apply -f $istioPath/Examples/Networking/primary-gateway.yaml -n primary --context=$CTX_CLUSTER1

#Generate Load - e.g. JMeter or Fortio or any other Laod Testing client

#Deploy Fortio client
kubectl apply -f $istioPath/Examples/HttpBin/sample-client/fortio-deploy.yaml -n primary --context=$CTX_CLUSTER1

#Make calls from Fortio client
export FORTIO_POD=$(kubectl get pods -l app=fortio -o 'jsonpath={.items[0].metadata.name}')
kubectl exec "$FORTIO_POD" -c fortio -- /usr/bin/fortio curl -quiet http://httpbin:8000/get

#Check Routing behaviour
#Observer many calls being failed and circuit is broken and joined automatically
#Change parameters in the $istioPath/Examples/Networking/httpbin-destination-rule.yaml file
#Play around and see the chnage in the behaviour

=============================================================================

Service Mirroring or Shadowing
=============================================================================
#Service Mirroring or Shadowing
#Create Secondary Cluster - CLI or Portal
export CTX_CLUSTER2=secondary

#Connect to Public AKS Cluster with Primary Context
az aks get-credentials -g $primaryResourceGroup -n $secondaryClusterName --context $CTX_CLUSTER2

kubectl config use-context $CTX_CLUSTER2

#Check Cluster Health - Secondary
kubectl get no --context=$CTX_CLUSTER2
kubectl get ns --context=$CTX_CLUSTER2
kubectl create namespace istio-system --context $CTX_CLUSTER2
kubectl create namespace secondary --context $CTX_CLUSTER2

#Install Istio CLI
#Select Default Istio Profile settings
#Ingress Gateway with Public IP Address
istioctl install --context=$CTX_CLUSTER2 --set profile=default -y

#Install Istio with custom configurations
#Ingress Gateway with Privae IP Address
#Another Publicly exposed LoadBalancer Service(L7) would be needed to access this Private IP
istioctl install --context=$CTX_CLUSTER2 -f $istioPath/Components/values-secondary.yaml -y

#Inject Istio into Secondary namespace of the cluster 
#This ensures sidecar container to be added for every deployment in this namespace
kubectl label namespace secondary istio-injection=enabled --context=$CTX_CLUSTER2

#Install Istio Addons
#This primarily installs all dependencies for observability by Istio viz. Grafana, Kiali dashboard etc.
kubectl apply -f $istioPath/Components/samples/addons --context=$CTX_CLUSTER2

kubectl get svc istio-ingressgateway -n istio-system
export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')

export GATEWAY_URL2=$INGRESS_HOST:$INGRESS_PORT
echo "$GATEWAY_URL2"

#Modify Application Gateway to accommodate Secondary Gateway to access Services deployed
Configure Private DNS Zone for Secondary Gateway with Istio Ingress Gateway IP
Add Listener for Secondary Gateway 
Add Http Settings for Secondary Gateway
Add Rules for Secondary Gateway Listener

#Need a Gateway to expose deployed services outside
#Check Routing definitions
#Replace <dns-name>
kubectl apply -f $istioPath/Examples/Networking/secondary-gateway.yaml -n secondary --context=$CTX_CLUSTER2

kubectl apply -f $istioPath/Examples/HelloWorld/helloworld-app-v2.yaml -n secondary --context=$CTX_CLUSTER2
kubectl get po -n secondary --context=$CTX_CLUSTER2

#Destination Rule
kubectl apply -f $istioPath/Examples/Networking/helloworld-v2-destination-rule.yaml -n secondary --context=$CTX_CLUSTER2

kubectl get svc -n secondary --context=$CTX_CLUSTER2
kubectl describe svc -n secondary --context=$CTX_CLUSTER2
kubectl get svc -A --context=$CTX_CLUSTER2

#Switch to the Primary Cluster
kubectl config use-context $CTX_CLUSTER1

#Check Routing definitions
kubectl apply -f $istioPath/Examples/Networking/primary-gateway.yaml -n primary --context=$CTX_CLUSTER1

#Deploy components so that Mirroring can work
#Replace <dns-name>
kubectl apply -f $istioPath/Examples/Networking/primary-serviceentry.yaml -n primary --context=$CTX_CLUSTER1

#Destination Rule
#Replace <dns-name>
kubectl apply -f $istioPath/Examples/Networking/helloworld-destination-rule.yaml -n primary --context=$CTX_CLUSTER1

kubectl get svc -n primary --context=$CTX_CLUSTER1
kubectl describe svc -n primary --context=$CTX_CLUSTER1
kubectl get svc -A --context=$CTX_CLUSTER1

#Call helloworld-v1
#Observe that all calls being replicated to helloworld-v2 of secondary cluster

Cleanup
=============================================================================
#Cleanup

#Uninstall Istio setup - primary cluster
istioctl x uninstall --set profile=default --purge --context=$CTX_CLUSTER1
kubectl delete namespace istio-system --context=$CTX_CLUSTER1

#Uninstall Istio setup - secondary cluster
istioctl x uninstall --set profile=default --purge --context=$CTX_CLUSTER2
kubectl delete namespace istio-system --context=$CTX_CLUSTER2





